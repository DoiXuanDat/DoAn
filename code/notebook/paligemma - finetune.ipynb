{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U git+https://github.com/huggingface/transformers.git datasets accelerate peft bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git config --global credential.helper store","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nif hf_token:\n    login(token=hf_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nds = load_dataset('HuggingFaceM4/VQAv2', split=\"train[:10%]\", trust_remote_code=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols_remove = [\"question_type\", \"answers\", \"answer_type\", \"image_id\", \"question_id\"]\nds = ds.remove_columns(cols_remove)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_ds = ds.train_test_split(test_size=0.1)\ntrain_ds = split_ds[\"train\"]\nval_ds = split_ds[\"test\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import PaliGemmaProcessor\nmodel_id = \"google/paligemma-3b-pt-224\"\nprocessor = PaliGemmaProcessor.from_pretrained(model_id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ndevice = \"cuda\"\n\nimage_token = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\ndef collate_fn(examples):\n  texts = [\"answer \" + example[\"question\"] for example in examples]\n  labels= [example['multiple_choice_answer'] for example in examples]\n  images = [example[\"image\"].convert(\"RGB\") for example in examples]\n  tokens = processor(text=texts, images=images, suffix=labels,\n                    return_tensors=\"pt\", padding=\"longest\",\n                    tokenize_newline_separately=False)\n\n  tokens = tokens.to(torch.bfloat16).to(device)\n  return tokens\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import PaliGemmaForConditionalGeneration\nimport torch\n\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n\nfor param in model.vision_tower.parameters():\n    param.requires_grad = False\n\nfor param in model.multi_modal_projector.parameters():\n    param.requires_grad = False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig\n\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_type=torch.bfloat16\n)\n\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\nargs = TrainingArguments(\n    num_train_epochs=5,\n    remove_unused_columns=False,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,           \n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    learning_rate=2e-5,\n    weight_decay=1e-6,\n    adam_beta2=0.999,\n    logging_steps=100,\n    eval_strategy=\"steps\",                 \n    eval_steps=500,                        \n    optim=\"adamw_8bit\",\n    save_strategy=\"steps\",\n    save_steps=1000,\n    push_to_hub=True,\n    save_total_limit=1,\n    output_dir=\"paligemma_vqav2_10pc\",\n    bf16=True,\n    report_to=[\"tensorboard\"],\n    dataloader_pin_memory=False,\n    load_best_model_at_end=True,           \n    metric_for_best_model=\"eval_loss\",    \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n        model=model,\n        train_dataset=train_ds ,\n        eval_dataset=val_ds,\n        data_collator=collate_fn,\n        args=args\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}